{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1493\n",
      "count/total:170/943=0.18027571580063625\n",
      "-----------------\n",
      "\n"
     ]
    },
    {
     "ename": "NotFoundError",
     "evalue": "Error code: 404 - {'error': {'message': 'The model `gpt-4` does not exist or you do not have access to it.', 'type': 'invalid_request_error', 'param': None, 'code': 'model_not_found'}}",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNotFoundError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 230\u001b[0m\n\u001b[0;32m    228\u001b[0m time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m5\u001b[39m)\n\u001b[0;32m    229\u001b[0m \u001b[38;5;66;03m# Make the request using the new `client.completions.create` method\u001b[39;00m\n\u001b[1;32m--> 230\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompletions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    231\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgpt-4\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Use model instead of engine\u001b[39;49;00m\n\u001b[0;32m    232\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprompt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m           \u001b[49m\u001b[38;5;66;43;03m# Pass your prompt\u001b[39;49;00m\n\u001b[0;32m    233\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m256\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m           \u001b[49m\u001b[38;5;66;43;03m# Maximum tokens for the response\u001b[39;49;00m\n\u001b[0;32m    234\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m            \u001b[49m\u001b[38;5;66;43;03m# Sampling temperature\u001b[39;49;00m\n\u001b[0;32m    235\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m                  \u001b[49m\u001b[38;5;66;43;03m# Top-p sampling\u001b[39;49;00m\n\u001b[0;32m    236\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m      \u001b[49m\u001b[38;5;66;43;03m# Penalize repeated tokens\u001b[39;49;00m\n\u001b[0;32m    237\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m       \u001b[49m\u001b[38;5;66;43;03m# Penalize introducing new topics\u001b[39;49;00m\n\u001b[0;32m    238\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m                       \u001b[49m\u001b[38;5;66;43;03m# Number of completions to generate\u001b[39;49;00m\n\u001b[0;32m    239\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    241\u001b[0m \u001b[38;5;66;03m# Extract the generated text from the response\u001b[39;00m\n\u001b[0;32m    242\u001b[0m predictions_1 \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mchoices[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mtext\n",
      "File \u001b[1;32md:\\NgJaBach\\PseudoC\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py:275\u001b[0m, in \u001b[0;36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    273\u001b[0m             msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing required argument: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquote(missing[\u001b[38;5;241m0\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    274\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[1;32m--> 275\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\NgJaBach\\PseudoC\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\completions.py:539\u001b[0m, in \u001b[0;36mCompletions.create\u001b[1;34m(self, model, prompt, best_of, echo, frequency_penalty, logit_bias, logprobs, max_tokens, n, presence_penalty, seed, stop, stream, stream_options, suffix, temperature, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[0;32m    510\u001b[0m \u001b[38;5;129m@required_args\u001b[39m([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprompt\u001b[39m\u001b[38;5;124m\"\u001b[39m], [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprompt\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m    511\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate\u001b[39m(\n\u001b[0;32m    512\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    537\u001b[0m     timeout: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m|\u001b[39m httpx\u001b[38;5;241m.\u001b[39mTimeout \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m|\u001b[39m NotGiven \u001b[38;5;241m=\u001b[39m NOT_GIVEN,\n\u001b[0;32m    538\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Completion \u001b[38;5;241m|\u001b[39m Stream[Completion]:\n\u001b[1;32m--> 539\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    540\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/completions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    541\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    542\u001b[0m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\n\u001b[0;32m    543\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodel\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    544\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mprompt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    545\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbest_of\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mbest_of\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    546\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mecho\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mecho\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    547\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfrequency_penalty\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    548\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlogit_bias\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogit_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    549\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlogprobs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    550\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_tokens\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    551\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mn\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    552\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpresence_penalty\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    553\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mseed\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    554\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstop\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    555\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstream\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    556\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstream_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    557\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msuffix\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43msuffix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    558\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtemperature\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    559\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtop_p\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    560\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43muser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    561\u001b[0m \u001b[43m            \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    562\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCompletionCreateParams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    563\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    564\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    565\u001b[0m \u001b[43m            \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\n\u001b[0;32m    566\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    567\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mCompletion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    568\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    569\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mStream\u001b[49m\u001b[43m[\u001b[49m\u001b[43mCompletion\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    570\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\NgJaBach\\PseudoC\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py:1280\u001b[0m, in \u001b[0;36mSyncAPIClient.post\u001b[1;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[0;32m   1266\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpost\u001b[39m(\n\u001b[0;32m   1267\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   1268\u001b[0m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1275\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1276\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[0;32m   1277\u001b[0m     opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(\n\u001b[0;32m   1278\u001b[0m         method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39mpath, json_data\u001b[38;5;241m=\u001b[39mbody, files\u001b[38;5;241m=\u001b[39mto_httpx_files(files), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions\n\u001b[0;32m   1279\u001b[0m     )\n\u001b[1;32m-> 1280\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[1;32md:\\NgJaBach\\PseudoC\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py:957\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[1;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[0;32m    954\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    955\u001b[0m     retries_taken \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m--> 957\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    958\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    959\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    960\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    961\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    962\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretries_taken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries_taken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    963\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\NgJaBach\\PseudoC\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py:1061\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[1;34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001b[0m\n\u001b[0;32m   1058\u001b[0m         err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mread()\n\u001b[0;32m   1060\u001b[0m     log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRe-raising status error\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m-> 1061\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_status_error_from_response(err\u001b[38;5;241m.\u001b[39mresponse) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1063\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_response(\n\u001b[0;32m   1064\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[0;32m   1065\u001b[0m     options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1069\u001b[0m     retries_taken\u001b[38;5;241m=\u001b[39mretries_taken,\n\u001b[0;32m   1070\u001b[0m )\n",
      "\u001b[1;31mNotFoundError\u001b[0m: Error code: 404 - {'error': {'message': 'The model `gpt-4` does not exist or you do not have access to it.', 'type': 'invalid_request_error', 'param': None, 'code': 'model_not_found'}}"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import time\n",
    "import numpy as np\n",
    "import json\n",
    "import random\n",
    "import openai \n",
    "\n",
    "# Define the class to hold argument values\n",
    "class args:\n",
    "    length_limit = 8\n",
    "    num_cand = 19\n",
    "    random_seed = 2023\n",
    "    api_key = \"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\"\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "rseed = args.random_seed\n",
    "random.seed(rseed)\n",
    "\n",
    "# Define utility functions for reading and writing JSON files\n",
    "def read_json(file):\n",
    "    with open(file) as f:\n",
    "        return json.load(f)\n",
    "\n",
    "def write_json(data, file):\n",
    "    with open(file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(data, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "# Read the MovieLens dataset\n",
    "data_ml_100k = read_json(\"ml_100k.json\")\n",
    "\n",
    "# Initialize OpenAI API key\n",
    "openai.api_key = args.api_key\n",
    "client = openai.OpenAI(api_key=args.api_key)\n",
    "\n",
    "# --- User-Item Mapping ---\n",
    "# Create a mapping of movies (items) to unique indices\n",
    "u_item_dict = {}\n",
    "u_item_p = 0\n",
    "for elem in data_ml_100k:\n",
    "    seq_list = elem[0].split(' | ')\n",
    "    for movie in seq_list:\n",
    "        if movie not in u_item_dict:\n",
    "            u_item_dict[movie] = u_item_p\n",
    "            u_item_p += 1\n",
    "print(len(u_item_dict))\n",
    "u_item_len = len(u_item_dict)\n",
    "\n",
    "# --- User Interaction Matrix ---\n",
    "# Create a binary user-item interaction matrix\n",
    "user_list = []\n",
    "for i, elem in enumerate(data_ml_100k):\n",
    "    item_hot_list = [0 for _ in range(u_item_len)]\n",
    "    seq_list = elem[0].split(' | ')\n",
    "    for movie in seq_list:\n",
    "        item_pos = u_item_dict[movie]\n",
    "        item_hot_list[item_pos] = 1\n",
    "    user_list.append(item_hot_list)\n",
    "user_matrix = np.array(user_list)\n",
    "\n",
    "# Compute User-User Similarity Matrix\n",
    "user_matrix_sim = np.dot(user_matrix, user_matrix.transpose())\n",
    "\n",
    "# --- Movie Popularity Dictionary ---\n",
    "# Count how many times each movie is watched across all users\n",
    "pop_dict = {}\n",
    "for elem in data_ml_100k:\n",
    "    seq_list = elem[0].split(' | ')\n",
    "    for movie in seq_list:\n",
    "        if movie not in pop_dict:\n",
    "            pop_dict[movie] = 0\n",
    "        pop_dict[movie] += 1\n",
    "\n",
    "# --- Item-Item Mapping ---\n",
    "# Create a mapping for items and their interaction with users\n",
    "i_item_dict = {}\n",
    "i_item_id_list = []\n",
    "i_item_user_dict = {}\n",
    "i_item_p = 0\n",
    "for i, elem in enumerate(data_ml_100k):\n",
    "    seq_list = elem[0].split(' | ')\n",
    "    for movie in seq_list:\n",
    "        if movie not in i_item_user_dict:\n",
    "            item_hot_list = [0. for _ in range(len(data_ml_100k))]\n",
    "            i_item_user_dict[movie] = item_hot_list\n",
    "            i_item_dict[movie] = i_item_p\n",
    "            i_item_id_list.append(movie)\n",
    "            i_item_p += 1\n",
    "        i_item_user_dict[movie][i] += 1\n",
    "\n",
    "# Create the Item-Item Interaction Matrix\n",
    "i_item_s_list = []\n",
    "for item in i_item_id_list:\n",
    "    i_item_s_list.append(i_item_user_dict[item])\n",
    "item_matrix = np.array(i_item_s_list)\n",
    "\n",
    "# Compute Item-Item Similarity Matrix\n",
    "item_matrix_sim = np.dot(item_matrix, item_matrix.transpose())\n",
    "\n",
    "# List of user indices\n",
    "id_list = list(range(0, len(data_ml_100k)))\n",
    "\n",
    "# --- User Filtering Function ---\n",
    "# Generate candidate movies using User-User Collaborative Filtering (UUCF)\n",
    "def sort_uf_items(target_seq, us, num_u, num_i):\n",
    "    candidate_movies_dict = {} \n",
    "    sorted_us = sorted(list(enumerate(us)), key=lambda x: x[-1], reverse=True)[:num_u]\n",
    "    dvd = sum([e[-1] for e in sorted_us])\n",
    "    for us_i, us_v in sorted_us:\n",
    "        us_w = us_v * 1.0 / dvd\n",
    "        us_elem = data_ml_100k[us_i]\n",
    "        us_seq_list = us_elem[0].split(' | ')\n",
    "\n",
    "        for us_m in us_seq_list:\n",
    "            if us_m not in target_seq:\n",
    "                if us_m not in candidate_movies_dict:\n",
    "                    candidate_movies_dict[us_m] = 0.\n",
    "                candidate_movies_dict[us_m] += us_w\n",
    "                \n",
    "    candidate_pairs = list(sorted(candidate_movies_dict.items(), key=lambda x: x[-1], reverse=True))\n",
    "    candidate_items = [e[0] for e in candidate_pairs][:num_i]\n",
    "    return candidate_items\n",
    "\n",
    "# --- Item Filtering Function ---\n",
    "# Generate candidate movies using Item-Item Collaborative Filtering (IICF)\n",
    "def soft_if_items(target_seq, num_i, total_i, item_matrix_sim, item_dict):\n",
    "    candidate_movies_dict = {} \n",
    "    for movie in target_seq:\n",
    "        sorted_is = sorted(list(enumerate(item_matrix_sim[item_dict[movie]])), key=lambda x: x[-1], reverse=True)[:num_i]\n",
    "        for is_i, is_v in sorted_is:\n",
    "            s_item = i_item_id_list[is_i]\n",
    "            \n",
    "            if s_item not in target_seq:\n",
    "                if s_item not in candidate_movies_dict:\n",
    "                    candidate_movies_dict[s_item] = 0.\n",
    "                candidate_movies_dict[s_item] += is_v\n",
    "    candidate_pairs = list(sorted(candidate_movies_dict.items(), key=lambda x: x[-1], reverse=True))\n",
    "    candidate_items = [e[0] for e in candidate_pairs][:total_i]\n",
    "    return candidate_items\n",
    "\n",
    "# --- Evaluation: Hit@N Metric ---\n",
    "# Evaluate recommendation accuracy and identify promising user sequences\n",
    "results_data_15 = []\n",
    "length_limit = args.length_limit\n",
    "num_u = 12\n",
    "total_i = args.num_cand\n",
    "count = 0\n",
    "total = 0\n",
    "cand_ids = []\n",
    "for i in id_list[:1000]:\n",
    "    elem = data_ml_100k[i]\n",
    "    seq_list = elem[0].split(' | ')\n",
    "    candidate_items = sort_uf_items(seq_list, user_matrix_sim[i], num_u=num_u, num_i=total_i)\n",
    "\n",
    "    if elem[-1] in candidate_items:\n",
    "        count += 1\n",
    "        cand_ids.append(i)\n",
    "    total += 1\n",
    "print(f'count/total:{count}/{total}={count * 1.0 / total}')\n",
    "print('-----------------\\n')\n",
    "\n",
    "# --- GPT-3 Integration ---\n",
    "# Templates for multi-step prompting\n",
    "temp_1 = \"\"\"\n",
    "Candidate Set (candidate movies): {}.\n",
    "The movies I have watched (watched movies): {}.\n",
    "Step 1: What features are most important to me when selecting movies (Summarize my preferences briefly)? \n",
    "Answer: \n",
    "\"\"\"\n",
    "\n",
    "temp_2 = \"\"\"\n",
    "Candidate Set (candidate movies): {}.\n",
    "The movies I have watched (watched movies): {}.\n",
    "Step 1: What features are most important to me when selecting movies (Summarize my preferences briefly)? \n",
    "Answer: {}.\n",
    "Step 2: Selecting the most featured movies from the watched movies according to my preferences (Format: [no. a watched movie.]). \n",
    "Answer: \n",
    "\"\"\"\n",
    "\n",
    "temp_3 = \"\"\"\n",
    "Candidate Set (candidate movies): {}.\n",
    "The movies I have watched (watched movies): {}.\n",
    "Step 1: What features are most important to me when selecting movies (Summarize my preferences briefly)? \n",
    "Answer: {}.\n",
    "Step 2: Selecting the most featured movies (at most 5 movies) from the watched movies according to my preferences in descending order (Format: [no. a watched movie.]). \n",
    "Answer: {}.\n",
    "Step 3: Can you recommend 10 movies from the Candidate Set similar to the selected movies I've watched (Format: [no. a watched movie - a candidate movie])?.\n",
    "Answer: \n",
    "\"\"\"\n",
    "\n",
    "# Iterate through promising user sequences and generate GPT-3 predictions\n",
    "count = 0\n",
    "total = 0\n",
    "results_data = []\n",
    "for i in cand_ids[:]:\n",
    "    elem = data_ml_100k[i]\n",
    "    seq_list = elem[0].split(' | ')[::-1]\n",
    "    candidate_items = sort_uf_items(seq_list, user_matrix_sim[i], num_u=num_u, num_i=total_i)\n",
    "    random.shuffle(candidate_items)\n",
    "\n",
    "    # Step 1: Generate GPT-3 input\n",
    "    input_1 = temp_1.format(', '.join(candidate_items), ', '.join(seq_list[-length_limit:]))\n",
    "\n",
    "    # Try GPT-3 API call with retries\n",
    "    try_nums = 5\n",
    "    kk_flag = 1\n",
    "    while try_nums:\n",
    "        try:\n",
    "            response = openai.Completion.create(\n",
    "                engine=\"text-davinci-003\",\n",
    "                prompt=input_1,\n",
    "                max_tokens=512,\n",
    "                temperature=0,\n",
    "                top_p=1,\n",
    "                frequency_penalty=0,\n",
    "                presence_penalty=0,\n",
    "                n=1,\n",
    "            )\n",
    "            try_nums = 0\n",
    "            kk_flag = 1\n",
    "        except Exception as e:\n",
    "            if 'exceeded your current quota' in str(e):\n",
    "                openai.api_key = args.api_key\n",
    "            time.sleep(1)\n",
    "            try_nums -= 1\n",
    "            kk_flag = 0\n",
    "\n",
    "    if kk_flag == 0:\n",
    "        time.sleep(5)\n",
    "        # Make the request using the new `client.completions.create` method\n",
    "        response = client.completions.create(\n",
    "            model=\"gpt-4\",  # Use model instead of engine\n",
    "            prompt=input_1,           # Pass your prompt\n",
    "            max_tokens=256,           # Maximum tokens for the response\n",
    "            temperature=0,            # Sampling temperature\n",
    "            top_p=1,                  # Top-p sampling\n",
    "            frequency_penalty=0,      # Penalize repeated tokens\n",
    "            presence_penalty=0,       # Penalize introducing new topics\n",
    "            n=1                       # Number of completions to generate\n",
    "        )\n",
    "\n",
    "        # Extract the generated text from the response\n",
    "        predictions_1 = response.choices[0].text\n",
    "\n",
    "    # Step 2: Refine with GPT-3 using additional prompts\n",
    "    input_2 = temp_2.format(', '.join(candidate_items), ', '.join(seq_list[-length_limit:]), predictions_1)\n",
    "    # (Continue with other prompts as in original code...)\n",
    "\n",
    "# Save results to a JSON file\n",
    "file_dir = f\"./results_multi_prompting_len{length_limit}_numcand_{total_i}_seed{rseed}.json\"\n",
    "write_json(results_data, file_dir)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
